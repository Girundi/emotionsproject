# emotionsproject
## Проект Распознавание эмоций на лекциях
Проект распазнования эмоций людей на лекциях с видеопотока ip-камер, использующий технологию свёрточных нейронных сетей (англ. convolutional neural networks). 

## Оглавление

* [Описание](#Описание)
* [Оглавление](#Оглавление)
* [Режимы работы](#Режимы-работы)
    * [Режимы получения видео потока](#Режимы-получения-видео-потока)
    * [Режимы вывода данных](#Режимы-вывода-данных)
    * [Таблицы](#Таблицы)
    * [Дополнительные режимы](#Дополнительные-режимы)
* [Установка](#Установка)
    * [Подготовка к использованию](#Подготовка-к-использованию)
    * [Поток данных](#Поток-данных)
* [Использование](#Использование)
    * [Запуск программы](#Запуск-программы)
    * [Реализация на сервере](#Реализация-на-сервере)
* [Ссылки](#Ссылки)

## Режимы работы

### Режимы получения видео потока

Объект класса Emanalisis имеет 3 режима получения видеопотока:

1) Использование стандартной вебкамеры на устройстве, на котором запущена программа. Если устройство не имеет таковых, будет вызвана ошибка
```python
analysator = Emanalisis(input_mode=0)
```
2) Использование потока с ip-камеры, адрес который указан в параметре channel. Перед использованием, необходимо удостовериться, что устройство, на котором запущена программа, и камера находятся в одной сети. 
```python
analysator = Emanalisis(input_mode=1, channel='172.223.45.67')
```
3) Анализ с видеое. В этом режиме в параметре channel указывается имя видеофайла, предпочтительно MP4.
```python
analysator = Emanalisis(input_mode=2, channel='video_file_name.mp4')
```

### Режимы вывода данных

Для того, чтобы получение обработанного видеопотока было в силе, необходимо передать в параметр record_video значение True.
3 режима передачи видеопотока:
1) Аугментированный поток. Лица выделены прямоугольниками, эмоции подписаны, внизу находится график подсчёта эмоций, вверху указан подсчёт числа распознаных людей(head_count) и коэффициента вовлеченности (attension_coef).
```python
analysator = Emanalisis(output_mode=0, record_video=True)
```
![Режим аугментированного видеопотока](images_for_docs/myimage.jpg)

2) Отдельный график подсчёта числа эмоций, выполненный на matplotlib.
```python
analysator = Emanalisis(output_mode=1, record_video=True)
```
![Режим отдельного графика](images_for_docs/myimage1.jpg)

3) Граф на тёмном фоне, с подсчётом head_count, attension_coef.
```python
analysator = Emanalisis(output_mode=2, record_video=True)
```
![Режим графа на чёрном фоне](images_for_docs/myimage2.jpg)

### Таблицы

При помощи библиотеки gspread, можно записывать сырые данные об эмоциях от нейронной сети. Но необходим аккаунт gmail для получения к ним доступа.
```python
analysator = Emanalisis(email_to_share='google@gmail.com')
analysator = Emanalisis(email_to_share=['mailbox@gmail.com', 'anothermailbox@gmail.com'])
```

### Дополнительные режимы

```python
analysator = Emanalysis(only_headcount=True, on_gpu=True, send_to_nvr=True, display=True)
```
only_headcount -- если равно True, уберает классификацию эмоций и оставляет только подсчёт числа людей

on_gpu -- если равно True, использует ускорение вычисленние на GPU. **!!!Не использовать на устройсвах без Nvidia GPU!!!** 

send_to_nvr -- если равно True, по завершении отправляет на сторонний сервис по ключу и URL, указанных в файле secrets.py

display -- если равно True, при вополнении открывает окно, где в реальном времени отображается выходной видео поток(даже если он не записывается). **!!!Не использовать при использовании на сервере!!!**

### Коэффициент вовлеченности

Коэффициент вовлеченности представляет собой отношение числа людей с самой частоmвстречающейся эмоцией к общему числу людей на данный момент. Предположение состоит в том, что чем большее число людей испытывает одну эмоцию, тем большее людей реагирует на один раздражитель, а значит вовлечены в учебный процесс. 

![Коэффициент вовлеченности](images_for_docs/CodeCogsEqn.gif)

В силу неточности классифицирующей сети и отсутствия учёта числа эмоций других, чем самая часто встречающаяся, делает коэффициент весьма ненадёжным, но предоставляет сколько-нибудь объективный метод оценки вовлеченности. Дальнейшая доработка метода или совмещение с другими показателями может увеличить точность оценки.

## Установка

1) Клонируйте или скачайте этот репозиторий.

2) Установите все необходимые библиотеки

3) Если вы планируете записывать данные в таблицы, пройдите [процедуру](https://gspread.readthedocs.io/en/latest/oauth2.html), указанную в gspread. Созданный json-файл положите в директорию проекта и укажите в файле secrets.py.

4) Если вы планируете передавать данные стороннему сервису, укажите его URL и API-ключ в файле secrets.py

5) Если вы планируете получать данные с ip-камер, то укажите её адрес, логин и пароль для доступа(если их нет, оставьте их пустыми), и ещё раз проверьте, находится ли устройство, на котором запускается программа, и камера в одной сети.

### Подготовка к использованию

6) Анализ происходит посредством функции run()
```python
a = Emanalysis()
a.run(filename=filename, stop_time=2120, fps_factor=25)
```

filename -- обязательный параметр, по имени которого даётся название таблицы и имя сохраняемого видео.

stop_time -- параметр, указывающий, когда в секундах должно остановиться выполнение функции run().

fps_factor -- параметр, указывающий какой Н-ый кадр берётся из видеопотока для анализа. Нужен для ускорения работы.

### Поток данных
![Поток данных](images_for_docs/dateflow_emotionsproject.png)

## Использование

### Запуск программы

Программа запускается из файла main.py.
```python
from core import Emanalisis
import platform
import os
import datetime
emails = ['iasizykh@miem.hse.ru', 'internet.prisoner59@gmail.com']

cam504 = '172.18.191.26'

a = Emanalisis(output_mode=2, input_mode=1, channel=cam504, record_video=True, on_gpu=False, display=False, email_to_share=emails, send_to_nvr=True)
while True:
    now = datetime.datetime.now()
    hour = int(now.strftime("%H"))
    if 9 <= hour:
        if now.strftime("%M") == "00" or now.strftime("%M") == "30":
            filename = now.strftime("%Y-%m-%d_") + (str(hour) if hour > 9 else "0" + str(hour)) \
                    + now.strftime("-%M")
            a.run(filename=filename, fps_factor=25, stop_time=1750)
```
В нашей реализации она запивсывает анализ указанной камеры каждые полчаса (50 секунды взяты из-за высокой задержки на детекцию лиц без использования GPU-ускорения). 

### Реализация на сервере

Для доступа к серверу, где запущена программа, используется ssh-соединение

```
ssh -i sbashrafkhonov.key sbashrafkhonov@217.73.60.165 -p 2522
```

Для доступа с сервера, где запущена программа, используеся OpenVPN туннель до сети с камерой. 

Программа с config-файлом OpenVPN'а смонтирован образ при помощи Docker'а.

Для запуска выполнения программы, находясь в директории /emotionsproject-master, команду

```
./start.sh
```

Для проверки работоспособности, проверьте наличае контейнера, находясь в папке /emotionsproject-master

```
docker ps
```

Если программа выполняется, то вывод будет примерно следующий:
```
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
ae34e7e65ea6        my_image            "/bin/sh -c 'openvpn…"   About an hour ago   Up About an hour                       my_container
```

Можно вносить изменения в файлы, кода программы, находящиеся в /emotionsproject-master/code. Чтобы эти изменения вошли в силу, нужно остановить выполнение и пересобрать образ.
```
docker stop my_container
docker rmi my_image
docker build my_image .
```
Наберитесь терпения. Это может занять минут 20-40.

Если программа выполняется как задуманно, то каждые 30 минут на указанные почтовые ящики должны приходить уведомления о создании таблиц.

## Ссылки

Сторонний сервис, куда отправляются видеофайлы -- [NVR MIEM](https://nvr.miem.hse.ru/).

Библиотека, для записи таблиц -- [Gspread](https://gspread.readthedocs.io/en/latest/index.html)

Решение для детекции лиц -- [RetinaFace](https://github.com/biubug6/Pytorch_Retinaface)

Набор данных, на котором обучалась модель для класификации -- [fer2013](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data)

